\section{Process' Perspective}
%\red{This perspective should clarify how code or other artifacts come from idea into the running system and everything that happens on the way.}

%\red{In particular, the following descriptions should be included:}

%\red{A complete description of stages and tools included in the CI/CD chains, including deployment and release of your systems.
%How do you monitor your systems and what precisely do you monitor?
%What do you log in your systems and how do you aggregate logs?
%Brief results of the security assessment and brief description of how did you harden the security of your system based on the analysis
%Applied strategy for scaling and upgrades
%In case you have used AI-assistants during your project briefly explain which system(s) you used during the project and reflect how it supported/hindered your process.}

This section provides an overview of our process, specifically detailing CI/CD chains, system monitoring, logging, security assessments, scaling and availability strategies, as well as the use of AI-assistants. %\textcolor{red}{er det fint med sådan en lille generic intro?}


%for this section see images/commit_checks,logging,monitor_bad,monitor_good
%https://codeclimate.com/github/TheisHS/test1-itu-minitwit -- link to codeclimate, everyone can see this
\subsection{CI/CD Pipeline}

Our pipeline for CI/CD is depicted in figure \ref{fig:cicd} and is managed on CircleCI. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/cicd.png}
    \caption{Visualisation of the CI/CD pipeline.}
    \label{fig:cicd}
\end{figure}

The source code on Github is linted using Hadolint and Shellcheck on each commit, and is analysed by the static code analysis tools mentioned in section \ref{sec:current-state} when a pull request is opened. 
When a pull request is merged into main, our \texttt{build\_and\_deploy} workflow is triggered, which can be seen in figure \ref{fig:build_and_deploy}. 
Releases have been done manually on Github once every Thursday evening.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/pipeline.png}
    \caption{The \texttt{build\_and\_deploy} workflow on CircleCI.}
    \label{fig:build_and_deploy}
\end{figure}

The decision to manage our CI/CD pipeline on CircleCI is based on the pros and cons in appendix \ref{app:ci_tool_choice}. 
Github Actions would also have been a good choice for this, but as there were no obvious downsides to either or, we went with the technology that we had not work with before as to gain experience with it.



% \subsubsection*{Tools and Technologies}
% Our tools and technologies relevant to the process is listed below, See Appendix \ref{app:technologies_and_tools} for a concise description of each tool.
% \begin{multicols}{2}
%     \begin{itemize}
%         \item \textbf{CircleCI}: Automates our build, test and deployments. 
%         \item \textbf{CodeClimate}: Used to improve code health.
%         \item \textbf{Github}: Version Control Management
%         \item \textbf{Hadolint}: A lint-tool that analyzes Dockerfiles.
%         \item \textbf{Shellcheck}: A shell script analysis tool.
%         \item \textbf{Sonarcloud}: Used to detect bugs and vulnerabilities.
%         \item \textbf{Vagrant}: Used initially for VM instantiating.
%     \end{itemize}
% \end{multicols}



% \subsubsection*{Deployment and Release Processes}
% Docker, feb 29


\subsection{System Monitoring}

%\subsubsection*{Monitoring Strategies and Tools}

%march 8,12,13 (check lecture notes for reactive vs proactive monitoring and what we have done)
In managing our system's health, we rely on two key tools: Prometheus and Grafana. Prometheus is a pull-based monitoring system that scrapes our defined metrics. We instrument the system with the Prometheus \textit{client\_golang} library, from which we create the metrics, register each metric using Prometheus' default register, finally creating HTTP endpoints exposing the metrics for Prometheus to scrape \cite{prometheusGolang}. Our approach is mainly whitebox monitoring in application-based metrics tracking the internal workings of the system. However, we do use a few blackbox monitoring elements by tracking specific endpoint responses. We use Grafana as a dashboarding tool that uses our Prometheus server as the data source to provide real-time insights of our measures.
%blackbox is from outside the application, a user's perspective where can track correct responses for specific requests.
%Reactive Monitoring: In reactive monitoring, the focus is on responding to issues after they occur.
%Proactive Monitoring: Proactive monitoring involves anticipating and preventing issues before they impact the system.

As from the Monitoring Maturity Model by James Turnbull \cite{MonitoringMaturityModel}, our monitoring strategy is mainly composed of reactive elements such as tracking specific error codes and status of specific user requests to respond promptly to issues that arise.

%As from the Monitoring Maturity Model by James Turnbull \cite{MonitoringMaturityModel}, our monitoring strategy combines proactive elements, such as monitoring uptime and tracking error rates to anticipate and prevent issues, with reactive elements, such as tracking specific error codes and status of specific user requests to respond promptly to issues that arise.


\subsubsection*{Key Metrics Monitored}
We have defined monitoring metrics for both the API and the web client. However, given the API's consistent workload from the simulation, this has been our primary focus. We have defined a total of 14 metrics for the API:
\begin{multicols}{2}
\begin{itemize}
    \item \textbf{http\_requests\_total}
    \item \textbf{database\_accesses\_total}
    \item \textbf{errors\_total}
    \item \textbf{get\_user\_id\_requests}
    \item \textbf{get\_user\_id\_requests\_failed}
    \item \textbf{follow\_requests}
    \item \textbf{follow\_requests\_failed}
    \item \textbf{unfollow\_requests}
    \item \textbf{unfollow\_requests\_failed}
    \item \textbf{post\_message\_requests}
    \item \textbf{post\_message\_requests\_failed}
    \item \textbf{not\_found}
    \item \textbf{bad\_request}
    \item \textbf{internal\_server\_error}
\end{itemize}
\end{multicols}

These monitoring metrics are visualised in a Grafana dashboard named \texttt{Minitwit\_Dashboard} as shown in Figure \ref{fig:monitor_good}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/monitor_good.png}
    \caption{Minitwit\_Dashboard - API Monitoring}
    \label{fig:monitor_good}
\end{figure}

The key metrics are categorised and displayed as follows:

\textbf{Total HTTP requests and Database accesses}, \textbf{Total Errors and Error Ratio}, \textbf{Error code distribution} of the total errors, and \textbf{Specific user request success/failure} (follow, unfollow, tweet). 

Additionally, the visualisation shows the uptime percentage of the API, Web Client, and Prometheus, which is derived from predefined Prometheus metrics.



\subsection{Log Aggregation Techniques}
To gain insight into how our system was being used, we have implemented a logging stack using Promtail, Loki and Grafana. 
The decision to not go for the otherwise popular ELK stack, is partly based on the fact that we could not get it to work, and partly based on the fact that we already used Grafana as a data visualisation tool for our monitoring setup.
% For logging we have used a setup with Promtail, Loki and Grafana. 
% Initially we went for an ELK stack, however after not being able to get it to work and discussing whether it was necessary to do it with that stack, we searched for alternatives and used a setup with Grafana which was already a part of our tech stack. 
% At first this did not work, but when we added Prometheus' AlertManager it worked. 
% The job of AlertManager is to remove duplicate log entries and route them to the correct receiver.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/logging.png}
    \caption{Grafana dashboard of logs.}
    \label{fig:logging}
\end{figure}

Our strategy for logging was to include the time of the event, which function was called and the parameters it was called with. 
When errors occur, we logged what the error is, and again which parameters were used, in order for us to more easily decode what the error might have been.
% Our techniques included logging general activities of the system with the purpose gaining of an insight into how the system was being used although this was not super useful in our context, but could have proven pivotal as an auditing mechanism if we had experienced database specific issues. 
% This happened on one of our log levels, namely INFO. 
% We also had a level for errors and logged all errors in our system.
As we had a suspicious amount of "User not found" errors on our \texttt{/fllws} endpoint, we created a widget on our dashboard that showed all unique usernames that were not found in the database. This enabled us to check whether these users actually did not exist.
The setup of our dashboard can be seen in figure \ref{fig:logging}.


\subsection{Security Assessment}
In our efforts to ensure the security of the ITU-MiniTwit system, we have utilized Snyk, Metasploit and SonarCloud, however, these tools have not identified any immediate vulnerabilities. To assess the security state of our system we have identified potential threats and performed a risk analysis.

\subsubsection*{Risk Identification}
Our primary asset is the ITU-MiniTwit web application, which includes frontend, backend, and database components.

One of the most common vulnerabilities for web applications involves injecting malicious SQL statements. Other threats consists of cross-site scripting (XSS) or misconfiguration, e.g. incorrectly configured servers, databases or applications.

From this we can construct several risk scenarios:

\begin{multicols}{2}
    \begin{itemize}
        \item \textbf{Scenario 1}: An attacker performs SQL injection on the web application to download sensitive user data.
        \item \textbf{Scenario 2}: An attacker exploits a cross-site scripting vulnerability to steal session cookies and impersonate users.
        \item \textbf{Scenario 3}: An attacker exploits a misconfiguration of the web server allowing unauthorized access to system resources.
        \item \textbf{Scenario 4}: A developer with legitimate access exposes sensitive data or sabotages the system, either intentionally or accidentally.
    \end{itemize} 
\end{multicols}

%"https://documents.ncsl.org/wwwncsl/Task-Forces/Cybersecurity-Privacy/IBM_Ponemon2017CostofDataBreachStudy.pdf" ved vi at "however we know from (kilde) that the the most common way to discover security failures is when a security incident happens.

\subsubsection*{Risk Analysis}
To analyze the identified risks we construct a risk matrix, see table \ref{tab:risk_matrix}.

\begin{table} [H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        Risk Scenario & Likelihood & Impact & Priority \\ \hline\hline
        SQL Injection & Low & High & Medium\\ \hline
        XSS & Low & Medium & Low \\ \hline 
        Misconfiguration & Medium & High & High \\ \hline 
        Developer error & Low & High & Medium \\ \hline
    \end{tabular}
    \caption{Risk matrix based on identified risks.}
    \label{tab:risk_matrix}
\end{table}

To mitigate the risk scenarios the following strategies are employed:
\begin{itemize}
    \item \textbf{Static code analysis}: The likelihood of our risk scenarios is covered in part by SonarCloud that has security rules setup, which analyzes the source code and allows detecting SQL Injection and XSS. %https://docs.sonarsource.com/sonarcloud/digging-deeper/security-related-rules/
    \item \textbf{Secrets}: To minimize the risk of exposing sensitive data, we use secrets in Docker swarm, GitHub and CircleCI. 
    \item \textbf{Automation}: Automatic configuration e.g. using tools like Vagrant ensures we avoid misconfigurations.
\end{itemize}

\subsubsection*{Security Hardening}
%HTTPS
%Docker secrets

For further security hardening we employ the HTTPS protocol. Our HTTPS certificate was obtained through CertBot. However, due to difficulties with the standard HTTP validation method, especially concerning load balancers as described in the article "How To Acquire a Let's Encrypt Certificate Using DNS Validation with acme-dns-certbot," \cite{certbotDigitalOceanIssues} we opted for an alternative approach. We used the acme-dns-certbot tool \cite{acmedns}, which interfaces CertBot with a third-party DNS server for validation instead of HTTP. This method is especially beneficial for issuing certificates for individual servers behind a load balancer, where traditional HTTP certificate validation would require setting validation files on each server.

\subsection{Scaling and Upgrade Strategy}
%docker swarm
Our monitoring dashboard detected several API crashes on our Digital Ocean node due to 100\% CPU usage. To improve service reliability and eliminate the single point of failure, we implemented scaling and replication using Docker Swarm.

\subsubsection*{Docker Swarm}
We chose Docker Swarm as our container orchestration platform because it is included by default with Docker and is simpler to use compared to alternatives like Kubernetes and OpenShift. Docker Swarm allowed us to set up a cluster consisting of manager and worker nodes, effectively creating a distributed system. In this setup, worker nodes run our stateful services, API and web client, independently, while the manager node handles service scheduling across the nodes using the routing mesh, Docker Swarm's built-in load balancing mechanism, and maintains the swarm state.

We first deployed the Docker Swarm as a separate cluster beside our single-node production server in Digital Ocean to ensure a transition that would not disrupt our live server. For each service, including our API, we added an additional replica in order to distribute the workload across multiple containers.

However, as the inherent load balancer, the routing mesh, of Docker Swarm is rather low-level, in which it from-the-shelf only redirects traffic to replicas when a node is “unhealthy”, or dead, we opted for a dedicated load balancer to complement the routing mesh by employing a more sophisticated traffic distribution. Simply redirecting traffic when a node is crashed  that initiated the whole focus on scaling and replication.

\subsubsection*{Nginx}
We chose to utilise Nginx as both reverse proxy and load balancer. This involved adding a new droplet in our Digital Ocean project outside the swarm to run an instance of Nginx, which we configured to handle a more sophisticated traffic distribution between the replicated services.

To configure Nginx, we separated the configuration files into two parts, one for the API and one for the web client. The configuration file for the API includes an upstream block that lists the IP addresses of the replicas in the swarm cluster along the API ports, allowing Nginx to direct traffic to the specified instances. Additionally we added server blocks for HTTP and HTTPS traffic, effectively redirecting all HTTP traffic to HTTPS. Similarly, the configuration file for the web client follows the same pattern but includes the IP addresses and service ports of the web client in the upstream block instead. 
%See appendix \textcolor{red}{X} for the configuration file of the API service.


\subsection{AI Tools Utilized}
To automate the production of easy or recurring code such as a new prometheus monitoring metric, generating skeleton code, code completion and helping us gaining a high-level understanding of the ITU-MiniTwit system, GitHub Copilot was used by multiple developers. It was quite bad at guessing the intended implementation logic of new features.